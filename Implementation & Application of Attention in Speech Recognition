# -*- coding: utf-8 -*-
"""hw4p2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M33WjoW4s5VqEsi5N-27CbEPNV2-DCt-
"""

# mount google drive
from google.colab import drive
drive.mount('/content/drive')

# #increase RAM in Google Colab, from 12 GB to 25 GB
  # a = []
  # while(1):
  #     a.append('1')

!unzip '/content/drive/My Drive/11785/11-785-s20-hw4p2.zip'

!pip install python-Levenshtein

# dataloader
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset 
from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pad_sequence
from torch.nn.utils.rnn import pack_sequence
from torch.nn.utils.rnn import pack_padded_sequence
from torch.nn.utils.rnn import pad_packed_sequence #only fun starts with pack has the 'enforce_sorted' arguments

'''
<sos> and <eos> are used to tell the encoder the start and end of the sequence
'''
LETTER_LIST = ['<pad>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', \
            'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '-', "'", '.', '_', '+', ' ','<sos>','<eos>']
'''
Loading all the numpy files containing the utterance information and text information
'''
def load_data():
    speech_train = np.load('/content/train_new.npy', allow_pickle=True, encoding='bytes')
    speech_valid = np.load('/content/dev_new.npy', allow_pickle=True, encoding='bytes')
    speech_test = np.load('/content/test_new.npy', allow_pickle=True, encoding='bytes')

    transcript_train = np.load('/content/train_transcripts.npy', allow_pickle=True,encoding='bytes')
    transcript_valid = np.load('/content/dev_transcripts.npy', allow_pickle=True,encoding='bytes')

    return speech_train, speech_valid, speech_test, transcript_train, transcript_valid


'''
Transforms alphabetical input to numerical input, replace each letter by its corresponding 
index from letter_list
'''
def transform_letter_to_index(transcript, LETTER_LIST):
    '''
    :param transcript :(N, ) Transcripts are the text input
    :param letter_list: Letter list defined above
    :return letter_to_index_list: Returns a list for all the transcript sentence to index
    '''
    letter_to_index_list = []
    for tsp in transcript:
        idxs = []
        idxs.append(LETTER_LIST.index('<sos>'))
        for i in range(len(tsp)):
            word = bytes.decode(tsp[i])
            for letter in word:
                idxs.append(LETTER_LIST.index(letter))
            if i != len(tsp)-1:
                idxs.append(LETTER_LIST.index(' '))
        idxs.append(LETTER_LIST.index('<eos>'))
        letter_to_index_list.append(idxs)
    return letter_to_index_list


'''
Optional, create dictionaries for letter2index and index2letter transformations
'''
def create_dictionaries(letter_list):
    letter2index = dict()
    index2letter = dict()
    return letter2index, index2letter


class Speech2TextDataset(Dataset):
    '''
    Dataset class for the speech to text data, this may need some tweaking in the
    getitem method as your implementation in the collate function may be different from
    ours. 
    '''
    def __init__(self, speech, text=None, isTrain=True):
        self.speech = speech
        self.isTrain = isTrain
        if (text is not None):
            self.text = text

    def __len__(self):
        return self.speech.shape[0]

    def __getitem__(self, index):
        if (self.isTrain == True):
            return torch.tensor(self.speech[index].astype(np.float32)), torch.tensor(self.text[index])
        else:
            return torch.tensor(self.speech[index].astype(np.float32))


def collate_train(batch_data):
    ### Return the padded speech and text data, and the length of utterance and transcript ###
    speech, text = zip(*batch_data)
    padded_speech = pad_sequence(speech, batch_first=True)
    padded_text = pad_sequence(text, batch_first=True)
    len_utterance = torch.LongTensor([len(s) for s in speech])
    len_transcript = torch.LongTensor([len(t) for t in text])
    return padded_speech, padded_text, len_utterance, len_transcript


def collate_test(batch_data):
    ### Return padded speech and length of utterance ###
    padded_speech = pad_sequence(batch_data, batch_first=True)
    len_utterance = torch.LongTensor([len(s) for s in batch_data])
    return padded_speech, len_utterance

# toy data
    # import numpy as np
    # speech_train = np.load('/content/train_new.npy', allow_pickle=True, encoding='bytes')
    # transcript_train = np.load('/content/train_transcripts.npy', allow_pickle=True,encoding='bytes')
    # character_text_train = transform_letter_to_index(transcript_train, LETTER_LIST)

    # train_dataset = Speech2TextDataset(speech_train, character_text_train)
    # train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_train)

    # for batch_num, (padded_speech, padded_text, len_utterance, len_transcript) in enumerate (train_loader):
    #     if batch_num==0:
    #         batch_speech = padded_speech
    #         batch_lens = len_utterance
    #         batch_text = padded_text
    #         batch_lens_text = len_transcript

# print(batch_speech.shape) # torch.Size([64, 1515, 40])
# train_loader.__len__() #387
# train_dataset.__len__() #24724

# test of encoder pass
    #     # pblstm = pBLSTM(40, 16)
    #     # pack_x = pack_padded_sequence(batch_speech, batch_lens, enforce_sorted=False, batch_first=True)
    #     # pack_out = pblstm(pack_x)
    #     # out, _ = pad_packed_sequence(pack_out, batch_first=True)
    #     # print(out.shape)

    #     # pblstm = pBLSTM(16, 32)
    #     # pack_out = pblstm(pack_out)
    #     # out, _ = pad_packed_sequence(pack_out, batch_first=True)
    #     # print(out.shape)
# test of attention pass
    #     # batch_size = 5
    #     # timesteps = 4
    #     # key_size = 3
    #     # value_size = 3

    #     # query = torch.ones(batch_size, key_size)
    #     # k = torch.ones(batch_size,timesteps, key_size)
    #     # v = torch.ones(batch_size,timesteps, value_size)
    #     # # lens = torch.ones(batch_size)*timesteps
    #     # lens = torch.tensor([4,4,3,2,1])

    #     # result = Attention()(query, k, v, lens)
    #     # # print("**Example Expected:**\n{result}".format(result))
    #     # for r in result:
    #     #     print(r.shape)
    # toy key & value 
    # encoder = Encoder()
    # key,value = encoder(batch_speech, batch_lens)

# test of decoder pass
    # vocab_size=len(LETTER_LIST)
    # hidden_dim=512
    # value_size=128
    # key_size=128
    # isAttended=True
    # embed = nn.Embedding(vocab_size, hidden_dim, padding_idx=0)
    # lstm1 = nn.LSTMCell(input_size=hidden_dim + value_size, hidden_size=hidden_dim)
    # lstm2 = nn.LSTMCell(input_size=hidden_dim, hidden_size=key_size)
    # attention = Attention()
    # batch_size = key.shape[0] #key :(N, T, key_size)
    # max_len =  batch_text.shape[1] # 228
    # embeddings = embed(batch_text) #torch.Size([N, T, emb])
    # hidden_states = [None, None]
    # # prediction = torch.zeros(batch_size,1).to(DEVICE)
    # prediction = torch.from_numpy(np.ones((batch_size,1))*LETTER_LIST.index('<sos>'))
    # lens  = torch.LongTensor([len(t) for t in batch_text])
    # print(embeddings.shape)

    # i = 0
    # char_embed = embeddings[:,i,:] # (N, key_size)
    # context = torch.zeros(batch_size, value_size)
    # inp = torch.cat([char_embed, context], dim=1)
    # # print(inp.shape)
    # hidden_states[0] = lstm1(inp, hidden_states[0])

    # inp_2 = hidden_states[0][0]
    # hidden_states[1] = lstm2(inp_2, hidden_states[1])

    # # ### Compute attention from the output of the second LSTM Cell ###
    # output = hidden_states[1][0]
    # context, _ = attention(output, key, value, lens)
    # # print(output.shape)
    # # print(context.shape)
    # character_prob = nn.Linear(key_size + value_size, vocab_size)
    # prediction = character_prob(torch.cat([output, context], dim=1))
    # print(prediction.unsqueeze(1).shape)

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
DEVICE

from torch.autograd import Variable
class LockedDropout(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x, dropout=0.5):
        if not self.training or not dropout:
            return x
        m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - dropout)
        mask = Variable(m, requires_grad=False) / (1 - dropout)
        mask = mask.expand_as(x)
        return mask * x

import torch
import torch.nn as nn
import torch.nn.utils as utils

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

class Attention(nn.Module):
    '''
    Attention is calculated using key, value and query from Encoder and decoder.
    Below are the set of operations you need to perform for computing attention:
        energy = bmm(key, query)
        attention = softmax(energy)
        context = bmm(attention, value)
    '''
    def __init__(self):
        super(Attention, self).__init__()

    def forward(self, query, key, value, lens):
        '''
        :param query :(N, context_size) Query is the output of LSTMCell from Decoder
        :param key: (N, key_size) Key Projection from Encoder per time step
        :param value: (N, value_size) Value Projection from Encoder per time step
        :param lens: (N,) length of each sequence # utterance length
        :return output: Attended Context (N, value_size)
        :return attention_mask: Attention mask that can be plotted  
        '''
        # key: (N, T, key_size), query:(N, key_size)
        energy = torch.bmm(key, query.unsqueeze(2)).squeeze(2)
        attention = nn.functional.softmax(energy, dim=1).to(DEVICE) #(N, T)
        pos = torch.cat([torch.arange(key.size(1)).reshape(1,-1) for i in range(key.size(0))],dim= 0).to(DEVICE)
        lens = lens.to(DEVICE)
        mask = (pos<lens.unsqueeze(1))*1 #(N,T)
        # mask = mask.to(DEVICE)
        masked_atten = nn.functional.normalize(attention*mask, dim=1)
        # attention unsqueeze: (N,1,T), value: (N,T,value_size)
        context = torch.bmm(masked_atten.unsqueeze(1), value).squeeze(1) 

        return context, masked_atten


class pBLSTM(nn.Module):
    '''
    Pyramidal BiLSTM
    The length of utterance (speech input) can be hundereds to thousands of frames long.
    The Paper reports that a direct LSTM implementation as Encoder resulted in slow convergence,
    and inferior results even after extensive training.
    The major reason is inability of AttendAndSpell operation to extract relevant information
    from a large number of input steps.
    '''
    def __init__(self, input_dim, hidden_dim):
        super(pBLSTM, self).__init__()
        self.blstm = nn.LSTM(input_size=input_dim*2, hidden_size=hidden_dim//2, 
                             num_layers=1, batch_first = True, bidirectional=True)
        self.lockdrop = LockedDropout()

    def forward(self, pack_x, dropout_rate):
        '''
        :param pack_x :(N, T) input to the pBLSTM, original dimension of x should be (N=batch_size,T=seq_len,fea_size=40)
        :return output: (N, T, H) encoded sequence from pyramidal Bi-LSTM 
        '''
        encoded_input, encoded_input_lengths = pad_packed_sequence(pack_x, batch_first=True)
        # print("before reshape:",encoded_input.shape)
        if encoded_input.size(1)%2 == 0:
            reshape_encode = encoded_input.reshape(-1, encoded_input.size(1)//2, encoded_input.size(2)*2)  
            # print("even", reshape_encode.shape)
        else:
            reshape_encode = encoded_input[:,:-1,:].reshape(encoded_input.size(0), encoded_input.size(1)//2, encoded_input.size(2)*2)
            # print("odd", reshape_encode.shape)
        encoded_input_lengths = encoded_input_lengths//2
        reshape_encode  = self.lockdrop(reshape_encode, dropout_rate)

        reshape_pack_encode = pack_padded_sequence(reshape_encode, encoded_input_lengths,\
                                                   batch_first=True, enforce_sorted=False)
        pack_out, final_state = self.blstm(reshape_pack_encode)
        return pack_out


class Encoder(nn.Module):
    '''
    Encoder takes the utterances as inputs and returns the key and value.
    Key and value are nothing but simple projections of the output from pBLSTM network.
    '''
    def __init__(self, input_dim=40, hidden_dim=512, value_size=128, key_size=128):
        super(Encoder, self).__init__()
        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim//2, 
                            num_layers=1, batch_first=True, bidirectional=True)
        
        ### Add code to define the blocks of pBLSTMs! ### 3 pBLSTM
        self.pBLSTM1 = pBLSTM(hidden_dim,256)
        self.pBLSTM2 = pBLSTM(256,256)
        self.pBLSTM3 = pBLSTM(256,256)
        
        self.key_network = nn.Linear(256, value_size)
        self.value_network = nn.Linear(256, key_size)
        self.lockdrop = LockedDropout()

    def forward(self, x, lens):
        # x = self.lockdrop(x, 0.1)
        rnn_inp = utils.rnn.pack_padded_sequence(x, lengths=lens, batch_first=True, 
                                                 enforce_sorted=False)
        outputs, _ = self.lstm(rnn_inp)
        
        ### Use the outputs and pass it through the pBLSTM blocks! ### 3 pBLSTM
        outputs = self.pBLSTM1(outputs, 0)
        outputs = self.pBLSTM2(outputs, 0.2)
        outputs = self.pBLSTM3(outputs, 0.2)

        linear_input, encoded_lens = utils.rnn.pad_packed_sequence(outputs,batch_first=True)
        # print(linear_input.shape) #torch.Size([batch_size, seq_len//8, 256])
        keys = self.key_network(linear_input)
        value = self.value_network(linear_input)

        return keys, value, encoded_lens


class Decoder(nn.Module):
    '''
    As mentioned in a previous recitation, each forward call of decoder deals with just one time step, 
    thus we use LSTMCell instead of LSLTM here.
    The output from the second LSTMCell can be used as query here for attention module.
    In place of value that we get from the attention, this can be replace by context we get from the attention.
    Methods like Gumble noise and teacher forcing can also be incorporated for improving the performance.
    '''
    def __init__(self, vocab_size=len(LETTER_LIST), emb_dim = 256, hidden_dim=512, value_size=128, key_size=128, isAttended=True):
        super(Decoder, self).__init__()
        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)
        self.lstm1 = nn.LSTMCell(input_size= emb_dim + value_size, hidden_size=hidden_dim)
        self.lstm2 = nn.LSTMCell(input_size=hidden_dim, hidden_size=key_size)

        self.isAttended = isAttended
        assert self.isAttended == True, "This network is designed to utilize attention."
        # if (isAttended == True):
        self.attention = Attention()
        self.teacher_forcing_rate = 0.5
        self.lockdrop = LockedDropout()

        self.character_prob = nn.Linear(key_size + value_size, vocab_size)

    def forward(self, key, values, encoded_lens, text=None, isTrain=True):
        '''
        :param key :(T, N, key_size) Output of the Encoder Key projection layer
        :param values: (T, N, value_size) Output of the Encoder Value projection layer
        :param text: (N, text_len) Batch input of text with text_length
        :param isTrain: Train or eval mode
        :return predictions: Returns the character perdiction probability 
        '''
        batch_size = key.shape[0] #key :(N, T, key_size)
        m = torch.distributions.bernoulli.Bernoulli(torch.tensor([self.teacher_forcing_rate]))

        if (isTrain == True):
            max_len =  text.shape[1]
            embeddings = self.embedding(text) #torch.Size([N, T, emb])
            embeddings = self.lockdrop(embeddings, 0.1)
        else:
            max_len = 250

        predictions = []
        hidden_states = [None, None]
        # prediction = torch.zeros(batch_size,1).to(DEVICE)
        batch_size = key.size(0)
        a = np.ones((batch_size,1))*LETTER_LIST.index('<sos>')
        a = a.astype(int)
        prediction = np.zeros((len(a), len(LETTER_LIST)))
        prediction[np.arange(len(a)), a]=1
        prediction = torch.from_numpy(prediction).float().to(DEVICE)
        predictions.append(prediction.unsqueeze(1))

        for i in range(max_len-1):
            # * Implement Gumble noise and teacher forcing techniques 
            # * When attention is True, replace values[i,:,:] with the context you get from attention.
            # * If you haven't implemented attention yet, then you may want to check the index and break 
            #   out of the loop so you do you do not get index out of range errors. 

            if (isTrain and m.sample() == torch.Tensor([0.])):
                char_embed = embeddings[:,i,:] # (N, emb)
            else:# test time or when it is p% of training(teacher forcing)
                char_embed = self.embedding(prediction.argmax(dim=-1)) #dim =-1 refers to the largest dim
            if i==0:
                context = torch.zeros(batch_size, values.size(2)).to(DEVICE) 

            inp = torch.cat([char_embed, context], dim=1)
            inp = self.lockdrop(inp.unsqueeze(1), 0.2).squeeze(1)
            hidden_states[0] = self.lstm1(inp, hidden_states[0])

            inp_2 = hidden_states[0][0]
            inp_2 = self.lockdrop(inp_2.unsqueeze(1), 0.2).squeeze(1)
            hidden_states[1] = self.lstm2(inp_2, hidden_states[1])

            ### Compute attention from the output of the second LSTM Cell ###
            output = hidden_states[1][0]
            context, _ = self.attention(output, key, values, encoded_lens)

            prediction = self.character_prob(torch.cat([output, context], dim=1))
            predictions.append(prediction.unsqueeze(1))
        return torch.cat(predictions, dim=1) #(N, text_len, vocab_size)


class Seq2Seq(nn.Module):
    '''
    We train an end-to-end sequence to sequence model comprising of Encoder and Decoder.
    This is simply a wrapper "model" for your encoder and decoder.
    '''
    def __init__(self, input_dim, vocab_size, hidden_dim, value_size=128, key_size=128, isAttended=False):
        super(Seq2Seq, self).__init__()
        self.encoder = Encoder(input_dim, hidden_dim)
        self.decoder = Decoder(vocab_size, hidden_dim)

    def forward(self, speech_input, speech_len, text_input=None, isTrain=True):
        key, value, encoded_lens = self.encoder(speech_input, speech_len)
        if (isTrain == True):
            predictions = self.decoder(key, value, encoded_lens, text_input)
        else:
            predictions = self.decoder(key, value, encoded_lens, text=None, isTrain=False)
        
        return predictions #(N, text_len, vocab_size)

# test train pass
    # DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
    # model = Seq2Seq(input_dim=40, vocab_size=len(LETTER_LIST), hidden_dim=512)
    # model.train() # otherwise the session would crash???
    # model.to(DEVICE)
    # optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    # criterion = nn.CrossEntropyLoss(reduction='none')
    # nepochs = 25
    # batch_size = 64 if DEVICE == 'cuda' else 1
    # batch_speech = batch_speech.to(DEVICE)
    # batch_text = batch_text.to(DEVICE)
    # batch_lens = batch_lens.to(DEVICE)
    # batch_lens_text = batch_lens_text.to(DEVICE)

    # print("num of words: {}".format(sum(batch_lens_text))) # 6678
    # print("len of speech:{}".format(sum(batch_lens))) # 43925

import time
import torch
from Levenshtein import distance
### Add Your Other Necessary Imports Here! ###

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

def train(model, train_loader, criterion, optimizer, epoch):
    model.train()
    model.to(DEVICE)
    # optimizer = optimizer.to(DEVICE)
    start = time.time()
    total_masked_loss = 0.
    num_words = 0
    perplexity = 0.

    # 1) Iterate through your loader
    for batch_num, (speech, text, len_speech, len_text) in enumerate (train_loader):
        # 2) Use torch.autograd.set_detect_anomaly(True) to get notices about gradient explosion
            
            # 3) Set the inputs to the device.
            speech = speech.to(DEVICE)
            text = text.to(DEVICE)
            len_speech = len_speech.to(DEVICE)
            len_text = len_text.to(DEVICE)

            optimizer.zero_grad()
            # model.zero_grad()

            # 4) Pass your inputs, and length of speech into the model.
            predictions = model(speech, len_speech, text, isTrain=True)

            # 5) Generate a mask based on the lengths of the text to create a masked loss. 
            # 5.1) Ensure the mask is on the device and is the correct shape.
            batch_size = text.size(0)
            pos = torch.cat([torch.arange(text.size(1)).reshape(1,-1) for i in range(batch_size)],dim= 0).to(DEVICE)
            mask = (pos<len_text.unsqueeze(1)) #(N,T)
            mask.to(DEVICE)
            assert mask.size(0) == batch_size, "mask dimension 0 is wrong"
            assert mask.size(1) == text.size(1), "mask dimension 1 is wrong"

            # 6) If necessary, reshape your predictions and origianl text input 
            # 6.1) Use .contiguous() if you need to. 
            predictions = predictions.view(predictions.size(0)*predictions.size(1), -1)
            text = text.view(text.size(0)*text.size(1))
            assert predictions.size(0) == text.size(0),"prediction dimension 0 is not equal with text dimension 0"

            # 7) Use the criterion to get the loss.
            loss = criterion(predictions, text)

            # 8) Use the mask to calculate a masked loss. 
            mask = mask.view(-1)
            masked_loss = loss[mask]
            # print("num of words: {}".format(sum(len_text)))
            # print("length of masked_loss: {}".format(masked_loss.shape))

            # 9) Run the backward pass on the masked loss. 
            avg_loss = torch.mean(masked_loss)
            avg_loss.backward()

            # 10) Use torch.nn.utils.clip_grad_norm(model.parameters(), 2)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 2)

            # 11) Take a step with your optimizer
            optimizer.step()

            # 12) Normalize the masked loss
            # total_masked_loss = total_masked_loss +sum(torch.nn.functional.normalize(masked_loss, dim=0))
            # total_masked_loss = total_masked_loss +sum(masked_loss)
            # num_words = num_words+sum(len_text)
            # perplexity = perplexity + torch.exp(masked_loss.sum()/len_text.sum()).item()

            # 13) Optionally print the training loss after every N batches
            # if batch_num%100==0 and batch_num !=0:
            #    end  = time.time()
            #    print("finish {}th batches, time elapsed: {} mins".format(batch_num,(end-start)/60))
    
    #update the teacher forcing rate
    print("teacher forcing rate is {}".format(model.decoder.teacher_forcing_rate))

    if model.decoder.teacher_forcing_rate<0.9:
        model.decoder.teacher_forcing_rate = model.decoder.teacher_forcing_rate + 0.05

    # perplexity = torch.exp(total_masked_loss/num_words)
    end = time.time()
    print("finish epoch {}, time elapsed {:.3f} min".format(epoch+1, (end-start)/60))
    print("learning rate: {:.4f}".format(optimizer.param_groups[0]['lr']))
    # print("perplexity is {}".format(perplexity))


def test(model, test_loader):
    total_pred_texts = []
    for batch_num, (padded_speech, len_utterance) in enumerate (test_loader):
        model.eval()
        model.to(DEVICE)
        padded_speech = padded_speech.to(DEVICE)
        len_utterance = len_utterance.to(DEVICE)

        predictions = model(padded_speech, len_utterance, None, isTrain=False)
        out_indices = []
        for i in range(predictions.size(0)):
            idx = [torch.argmax(predictions[i, 0, :])]   
            t = 1
            while (idx[-1]!=LETTER_LIST.index('<eos>') and t!=250):
                x = torch.argmax(predictions[i, t, :])
                if x!=idx[-1]:
                    idx.append(x)
                t = t +1
            out_indices.append(idx)

        # pred_texts = []
        for i in range(predictions.size(0)):
            text = ''
            for t in range(len(out_indices[i])):
                if(out_indices[i][t]==0 or \
                   out_indices[i][t]==LETTER_LIST.index('<eos>') or \
                   out_indices[i][t]==LETTER_LIST.index('<sos>')):
                    pass
                else:
                    text = text + LETTER_LIST[out_indices[i][t]]
            # pred_texts.append(text)
            total_pred_texts.append(text) 
    return total_pred_texts


def val(model, val_loader, epoch, transcript_valid):
    total_pred_texts = test(model, val_loader)
    distances = list(map(distance, total_pred_texts, transcript_valid))
    # print(len(total_pred_texts), len(transcript_valid))
    print("epoch {}, validation distance {:.3f}".format(epoch+1, np.mean(distances)))
    return np.mean(distances)

# if we are retraining the saved model, uncomment the block below
model = Seq2Seq(input_dim=40, vocab_size=len(LETTER_LIST), hidden_dim=512).cpu()
PATH = '/content/drive/My Drive/11785/hw4p2.pt'
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.9, patience=1)

checkpoint = torch.load(PATH)
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
epoch = checkpoint['epoch']
loss = checkpoint['loss']
model.decoder.teacher_forcing_rate = 0.85

for state in optimizer.state.values():
    for k, v in state.items():
        if torch.is_tensor(v):
            state[k] = v.cuda()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

# from models import Seq2Seq
# from train_test import train, test
# from dataloader import load_data, collate_train, collate_test, transform_letter_to_index, Speech2TextDataset

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

LETTER_LIST = ['<pad>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', \
               'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '-', "'", '.', '_', '+', ' ','<sos>','<eos>']

# def main():
print("exec here")

# if it is the first time training the model, uncomment the block below
# model = Seq2Seq(input_dim=40, vocab_size=len(LETTER_LIST), hidden_dim=512)
# optimizer = optim.Adam(model.parameters(), lr=0.001)
# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.9, patience=1)

criterion = nn.CrossEntropyLoss(reduction='none')
nepochs = 10 # 25+10
batch_size = 64 if DEVICE == 'cuda' else 1

speech_train, speech_valid, speech_test, transcript_train, transcript_valid = load_data()
character_text_train = transform_letter_to_index(transcript_train, LETTER_LIST)
character_text_valid = transform_letter_to_index(transcript_valid, LETTER_LIST)
words_transcript_valid = [list(map(bytes.decode, text)) for text in transcript_valid]
str_transcript_valid = [' '.join(words) for words in words_transcript_valid]

train_dataset = Speech2TextDataset(speech_train, character_text_train)
val_dataset = Speech2TextDataset(speech_valid, character_text_valid, isTrain=False)
test_dataset = Speech2TextDataset(speech_test, None, False)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_train)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_test)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_test)
print("num of batches:{}".format(train_loader.__len__()))

for epoch in range(nepochs):
    train(model, train_loader, criterion, optimizer, epoch)
    val_loss = val(model, val_loader, epoch, str_transcript_valid)
    scheduler.step(val_loss)
    # plot_ 
    # save model
    if epoch%1==0:
        torch.save({
        'epoch':epoch,
        'model_state_dict':model.state_dict(),
        'optimizer_state_dict':optimizer.state_dict(),
        'scheduler_state_dict':scheduler.state_dict(),
        'loss':val_loss,          
    },"./hw4p2.pt")
    # test(model, test_loader)


# if __name__ == '__main__':
#     main()

import pandas as pd
out_str = test(model, test_loader)
idx = [ i for i in range(len(speech_test))]
test_prediction = pd.DataFrame(list(zip(idx,out_str)))

test_prediction.columns = ['id','Predicted']
test_prediction.to_csv("submission.csv", index=False)
