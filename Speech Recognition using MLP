# -*- coding: utf-8 -*-
"""
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LUXAXSG4E6mlOU2D_EvaSzhqpAE5qe1C
"""

from google.colab import drive
drive.mount('/content/drive/')

!ls "/content/drive/My Drive/11785"

!cd /content/drive

# -*- coding: utf-8 -*-
"""
@author: Cynthia Long

"""
import time
import torch
import numpy as np
import pandas as pd
import torch.nn as nn
import torch.optim as optim


#%%
#the model is trained on the google colab
train_data = np.load('/content/drive/My Drive/11785/train.npy',allow_pickle=True)
train_labels = np.load('/content/drive/My Drive/11785/train_labels.npy',allow_pickle=True)

test_data = np.load('/content/drive/My Drive/11785/test.npy',allow_pickle=True)

val_data = np.load('/content/drive/My Drive/11785/dev.npy',allow_pickle=True)
val_labels = np.load('/content/drive/My Drive/11785/dev_labels.npy',allow_pickle=True)

train_idx_map = {}
test_idx_map = {}
val_idx_map = {}

def build_idx(train_data):
    idx_map = {}
    past_sum = 0
    for utter_idx in range(len(train_data)):
    #frame_idx is the index for the frames in each utterance, the second dimension of the data
        for frame_idx in range(len(train_data[utter_idx])):
            i = past_sum+frame_idx
            idx_map[i] = (utter_idx, frame_idx)
        past_sum = past_sum +len(train_data[utter_idx])
    return idx_map
  
train_idx_map = build_idx(train_data)
val_idx_map = build_idx(val_data)
test_idx_map = build_idx(test_data)

#def pad_frames(data,idx_map):
#    data_list = []
#    for index in range(len(train_idx_map.keys())):
#        utter_idx, frame_idx = idx_map[index]
#            #padding near frames before the frame
#        padded_utter = np.concatenate((np.zeros((12,40)),data[utter_idx],np.zeros((12,40))))
#        data_list.append(padded_utter[frame_idx-12+12:frame_idx+13+12,:].ravel())       
#    return data_list
#train_data_list = pad_frames(train_data, train_idx_map)
#
#val_data_list = pad_frames(val_data,val_idx_map)
#
#test_data_list = pad_frames(test_data,test_idx_map)

#del train_data
#gc.collect()

#%%
#define the dataloader for train_data, train_label and dev_data, dev_label 
class MyDataset(torch.utils.data.Dataset):
    def __init__(self, data, labels, idx_map):
        
        self.X = data
        self.Y = np.concatenate(labels)
        self.idx_map = idx_map
        
#        print("==========padding training/validation data==========")
#        start_time = time.time()
        #utter_idx is the index for the utterance, the first dimension of the data
        
    def __len__(self):
        return len(self.idx_map.keys())

    def __getitem__(self, index):
        #print("{} finished".format(index))
        utter_idx, frame_idx = self.idx_map[index]
        padded_utter = np.concatenate((np.zeros((12,40)),self.X[utter_idx],np.zeros((12,40))))       
        return torch.from_numpy(padded_utter[frame_idx-12+12:frame_idx+13+12,:].ravel()).float(), torch.tensor(self.Y[index]).long()
#%%
#define the dataloader for test data
class MyDatasetTest(torch.utils.data.Dataset):
    def __init__(self, data, idx_map):
        self.X = data
        #self.Y = labels
        self.idx_map = idx_map
        
#        print("==========padding training/validation data==========")
#        start_time = time.time()
        #utter_idx is the index for the utterance, the first dimension of the data

    def __len__(self):
        return len(self.idx_map.keys())

    def __getitem__(self, index):
        #print("{} finished".format(index))
        utter_idx, frame_idx = self.idx_map[index]
#        padded_data = np.take(self.X[utter_idx], \
#        [i+frame_idx for i in range(-12,13)], axis = 0, mode = 'clip').flatten()
        padded_utter = np.concatenate((np.zeros((12,40)),self.X[utter_idx],np.zeros((12,40))))       
        return torch.from_numpy(padded_utter[frame_idx-12+12:frame_idx+13+12,:].ravel()).float()
#%%
cuda = torch.cuda.is_available()
if cuda == False:
   print("==========Cuda not found!==========")
num_workers = 8 if cuda else 0 

test_dataset = MyDatasetTest(test_data, test_idx_map)
test_loader_args = dict(shuffle=False, batch_size=1, num_workers=num_workers, \
                         pin_memory=True) if cuda\
                    else dict(shuffle=True, batch_size=64)
test_loader = torch.utils.data.DataLoader(test_dataset, **test_loader_args)    
# Training
train_dataset = MyDataset(train_data, train_labels, train_idx_map)
train_loader_args = dict(shuffle=True, batch_size=1024, num_workers=num_workers, \
                         pin_memory=True) if cuda\
                    else dict(shuffle=True, batch_size=64)
train_loader = torch.utils.data.DataLoader(train_dataset, **train_loader_args)

val_dataset = MyDataset(val_data, val_labels, val_idx_map)
val_loader_args = dict(shuffle=True, batch_size=1024, num_workers=num_workers, \
                         pin_memory=True) if cuda\
                    else dict(shuffle=True, batch_size=64)
val_loader = torch.utils.data.DataLoader(val_dataset, **val_loader_args)

#%%
#build a baseline model
class HW1_MLP(nn.Module):
    def __init__(self, size_list):
        super(HW1_MLP, self).__init__()
        layers = []
        self.size_list = size_list
        for i in range(len(size_list) - 2):
            layers.append(nn.Linear(size_list[i],size_list[i+1]))
            layers.append(nn.ReLU())
            layers.append(nn.BatchNorm1d(num_features=size_list[i+1]))
        layers.append(nn.Linear(size_list[-2], size_list[-1]))
        self.net = nn.Sequential(*layers)
        self.net = self.net.float()

    def forward(self, x):
        return self.net(x)
#%%
model = HW1_MLP([40 * (2*12 + 1), 2048, 1024, 138])
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())
device = torch.device("cuda" if cuda else "cpu")
model.to(device)
print(model)
#%%
#use train_data and train_label to validate the model
def train_epoch(model, train_loader, criterion, optimizer):
    model.train()

    running_loss = 0.0
    
    start_time = time.time()
    for batch_idx, (data, target) in enumerate(train_loader):   
        optimizer.zero_grad()   # .backward() accumulates gradients
        data = data.to(device)
        target = target.to(device) # all data & model on same device

        outputs = model(data)
        loss = criterion(outputs, target)
        running_loss += loss.item()

        loss.backward()
        optimizer.step()
        #print("finish training batch {}".format(batch_idx))
    end_time = time.time()
    
    running_loss /= len(train_loader)
    print('Training Loss: ', running_loss, 'Time: ',end_time - start_time, 's')
    torch.save(model,'/content/drive/My Drive/11785/hw1_model.pkl') 
    #return running_loss
  
#%%
#use dev_data and dev_label to validate the model
def val_model(model, val_loader, criterion):
    with torch.no_grad():
        model.eval()

        running_loss = 0.0
        total_predictions = 0.0
        correct_predictions = 0.0

        for batch_idx, (data, target) in enumerate(val_loader):   
            data = data.to(device)
            target = target.to(device)

            outputs = model(data)

            _, predicted = torch.max(outputs.data, 1)
            total_predictions += target.size(0)
            correct_predictions += (predicted == target).sum().item()

            loss = criterion(outputs, target).detach()
            running_loss += loss.item()


        running_loss /= len(val_loader)
        acc = (correct_predictions/total_predictions)*100.0
        print('Testing Loss: ', running_loss)
        print('Testing Accuracy: ', acc, '%')
        #return running_loss, acc

#%%
def predict_model(model, test_loader):
    with torch.no_grad():
        model.eval()

        i = 0
        for batch_idx, data in enumerate(test_loader):
            data = data.to(device)
          
            outputs = model(data)
            _, predicted = torch.max(outputs.data, 1)
            if i==0:
                total_predicted = predicted
            else:
                total_predicted = torch.cat((total_predicted, predicted), 0)
            i = i+1
        print("finish predicting {} batches test samples".format(i))    
      
        return total_predicted      
#%%
n_epochs = 10
#Train_loss = []
#Val_loss = []
#Val_acc = []

for i in range(n_epochs):
    train_epoch(model, train_loader, criterion, optimizer)
    val_model(model, val_loader, criterion)
    print("==========finishi training {} epoch==========".format(i+1))
#    Train_loss.append(train_loss)
#    Val_loss.append(val_loss)
#    Val_acc.append(val_acc)
#    print('='*20)

"""# New Section"""

#model = torch.load('/content/drive/My Drive/11785/hw1_model.pkl')
def predict_model(model, test_loader):
    total_predicted = []
    with torch.no_grad():
        model.eval()

        for batch_idx, data in enumerate(test_loader):   
            data = data.to(device)
            outputs = model(data)

            _, predicted = torch.max(outputs.data, 1)
            total_predicted.append(predicted.cpu().numpy().astype(np.int32)) 
        return np.concatenate(total_predicted)      
test_labels = predict_model(model, test_loader)
#test_labels = test_labels.cpu().numpy()
prediction = pd.DataFrame(test_labels)
prediction.columns = ['label']
prediction.index.name = 'id'
prediction.to_csv('/content/drive/My Drive/11785/result.csv')
