# -*- coding: utf-8 -*-
"""

Automatically generated by Colaboratory.

Original file is located at
    ************************************
"""

# from google.colab import drive
# drive.mount('/content/drive')

N_STATES = 138
N_PHONEMES = N_STATES // 3
PHONEME_LIST = [
    "+BREATH+",
    "+COUGH+",
    "+NOISE+",
    "+SMACK+",
    "+UH+",
    "+UM+",
    "AA",
    "AE",
    "AH",
    "AO",
    "AW",
    "AY",
    "B",
    "CH",
    "D",
    "DH",
    "EH",
    "ER",
    "EY",
    "F",
    "G",
    "HH",
    "IH",
    "IY",
    "JH",
    "K",
    "L",
    "M",
    "N",
    "NG",
    "OW",
    "OY",
    "P",
    "R",
    "S",
    "SH",
    "SIL",
    "T",
    "TH",
    "UH",
    "UW",
    "V",
    "W",
    "Y",
    "Z",
    "ZH"
]

PHONEME_MAP = [
    '_',  # "+BREATH+"
    '+',  # "+COUGH+"
    '~',  # "+NOISE+"
    '!',  # "+SMACK+"
    '-',  # "+UH+"
    '@',  # "+UM+"
    'a',  # "AA"
    'A',  # "AE"
    'h',  # "AH"
    'o',  # "AO"
    'w',  # "AW"
    'y',  # "AY"
    'b',  # "B"
    'c',  # "CH"
    'd',  # "D"
    'D',  # "DH"
    'e',  # "EH"
    'r',  # "ER"
    'E',  # "EY"
    'f',  # "F"
    'g',  # "G"
    'H',  # "HH"
    'i',  # "IH"
    'I',  # "IY"
    'j',  # "JH"
    'k',  # "K"
    'l',  # "L"
    'm',  # "M"
    'n',  # "N"
    'G',  # "NG"
    'O',  # "OW"
    'Y',  # "OY"
    'p',  # "P"
    'R',  # "R"
    's',  # "S"
    'S',  # "SH"
    '.',  # "SIL"
    't',  # "T"
    'T',  # "TH"
    'u',  # "UH"
    'U',  # "UW"
    'v',  # "V"
    'W',  # "W"
    '?',  # "Y"
    'z',  # "Z"
    'Z',  # "ZH"
]

assert len(PHONEME_LIST) == len(PHONEME_MAP)
assert len(set(PHONEME_MAP)) == len(PHONEME_MAP)

PHONEME_MAP = [" "]+PHONEME_MAP
# len(PHONEME_MAP)

# #increase RAM in Google Colab, from 12 GB to 25 GB
# a = []
# while(1):
#     a.append('1')

import torch
from torch import nn
from torch.nn.utils.rnn import *
# import torch.nn.utils.rnn as rnn
from torch.utils.data import Dataset, DataLoader, TensorDataset
import numpy as np
import time
from torch.nn.utils.rnn import pad_sequence
import gc
# DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
cuda = torch.cuda.is_available()
DEVICE = torch.device("cuda" if cuda else "cpu")
DEVICE

# !git clone --recursive https://github.com/parlance/ctcdecode.git
# !pip install wget
# %cd ctcdecode
# !pip install .
# %cd ..

# !pip install python-Levenshtein

from Levenshtein import distance
print(distance('Lenvinsten','Levenshtein'))
distance('Levenshtein', 'Lenvinsten')

from ctcdecode import CTCBeamDecoder

decoder = CTCBeamDecoder([' ', 'A'], beam_width=4)
probs = torch.Tensor([[0.2, 0.8], [0.8, 0.2]]).unsqueeze(0)
print(probs.size())
out, _, _, out_lens = decoder.decode(probs, torch.LongTensor([2]))
print(out[0, 0, :out_lens[0, 0]])

class LinesDataset(Dataset):
    def __init__(self,lines, labels, line_lens, label_lens):
        labels = labels + 1
        labels = [torch.tensor(l) for l in labels] 
        self.labels = pad_sequence(labels,batch_first=True)
        lines=[torch.Tensor(x) for x in lines]
        self.lines = pad_sequence(lines,batch_first=True)
        self.line_lens = line_lens
        self.label_lens = label_lens
        del lines, labels, line_lens, label_lens
        gc.collect()

        # self.line_lens = lens
        # self.label_lens = label_lens

    def __getitem__(self,i):        
        return self.lines[i],  self.labels[i], self.line_lens[i], self.label_lens[i]


    def __len__(self):
        return len(self.lines)

# collate fn lets you control the return value of each batch
# for packed_seqs, you want to return your data sorted by length
def collate_lines(seq_list):
    inputs, targets = zip(*seq_list) # unzip the zipped var
    # lens = [len(seq) for seq in inputs]
    # seq_order = sorted(range(len(lens)), key=lens.__getitem__, reverse=True)#argsort
    # inputs = [inputs[i] for i in seq_order]
    # targets = [targets[i] for i in seq_order]
    return inputs, targets

train = np.load('/home/ubuntu/hw3p2/wsj0_train',allow_pickle=True)
train_label = np.load('/home/ubuntu/hw3p2/wsj0_train_merged_labels.npy', allow_pickle=True)
train_lens = torch.LongTensor([len(seq) for seq in train])
train_lens = (train_lens-3+2)//2
train_label_lens = torch.LongTensor([len(seq) for seq in train_label])

dev = np.load('/home/ubuntu/hw3p2/wsj0_dev.npy',allow_pickle=True)
dev_label = np.load('/home/ubuntu/hw3p2/wsj0_dev_merged_labels.npy', allow_pickle=True)
dev_lens = torch.LongTensor([len(seq) for seq in dev])
dev_lens = (dev_lens-3+2)//2
dev_label_lens = torch.LongTensor([len(seq) for seq in dev_label])

train_dataset = LinesDataset(train, train_label, train_lens, train_label_lens)
train_loader = DataLoader(train_dataset, shuffle=False, batch_size=64)
val_dataset = LinesDataset(dev, dev_label, dev_lens, dev_label_lens)
val_loader = DataLoader(val_dataset, shuffle=False, batch_size=64)

del train, train_label, train_lens, train_label_lens
del dev, dev_label, dev_lens. dev_label_lens
gc.collect()

class Model(nn.Module):
    def __init__(self, in_vocab = len(PHONEME_LIST)+1, out_vocab=len(PHONEME_LIST)+1, \
                 embed_size = 20, hidden_size =256, nlayers=7):
        super(Model, self).__init__()
        self.embed = nn.Conv1d(40, 20, 3, stride=2)
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=nlayers, bidirectional=True, batch_first=True)
        self.output = nn.Linear(hidden_size * 2, out_vocab)
    
    def forward(self, X, lengths):
        X = self.embed(X.transpose(1,2)).transpose(1,2)
        
        packed_X = pack_padded_sequence(X, lengths, enforce_sorted=False, batch_first=True)      
        packed_out = self.lstm(packed_X)[0]
        out, out_lens = pad_packed_sequence(packed_out, batch_first=True)
        
        # Log softmax after output layer is required since`nn.CTCLoss` expects log probabilities.
        out = self.output(out).log_softmax(2)
        return out, out_lens

ctcdecoder = CTCBeamDecoder(['$'] * (len(PHONEME_MAP)), beam_width=10, log_probs_input=True)

def val(model, val_loader):
  out_str = []
  val_loss = 0.0
  with torch.no_grad():
    model.eval()
    for batch_idx, (inputs, targets, input_lens, target_lens) in enumerate(val_loader):
      inputs = inputs.to(DEVICE)
      targets = targets.to(DEVICE)
      # target_lens = torch.LongTensor([len(seq) for seq in targets])
      # print(target_lens)
      out, out_lens = model(inputs, input_lens)
      
      test_Y, _, _, test_Y_lens = ctcdecoder.decode(out, out_lens)   
      len_inputs = len(inputs) 
      for i in range(len_inputs):
          # For the i-th sample in the batch, get the best output
          best_seq = test_Y[i, 0, :test_Y_lens[i, 0]]
          best_pron = ''.join(PHONEME_MAP[j] for j in best_seq)
          targets_i = targets[i][:target_lens[i]]
          target_pron = ''.join(PHONEME_MAP[j] for j in targets_i)
          val_loss = val_loss + distance(best_pron, target_pron)
          # print(test[i], '->', best_pron)
          # out_str.append(best_pron)
          del best_seq, best_pron, target_pron
      del inputs, input_lens, targets, out, out_lens
      gc.collect()
      torch.cuda.empty_cache()
    return val_loss/(val_dataset.__len__())
# print(val(ctcmodel,val_loader))

# save the model, and then do beam search
torch.manual_seed(11785)  
ctcmodel = Model()
# model.cuda()
ctcmodel.to(DEVICE)
# criterion = nn.CTCLoss()
# criterion = criterion.to(DEVICE)
criterion = nn.CTCLoss(zero_infinity = True)
optimizer = torch.optim.Adam(ctcmodel.parameters(), lr=1e-3, weight_decay =5e-5)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, patience=1)
PATH = "CTCNetwork.pt"
def train_epoch(model, train_loader, val_loader, epoch):
  for i in range(epoch):
    print("epoch {}".format(i+1))
    model.train()
    
    total_loss = 0.0
    before = time.time()
    # print("training", len(train_loader), "number of batches")
    for batch_idx, (inputs, targets, input_lens, target_lens) in enumerate(train_loader):
        # if batch_idx == 0:
        #     first_time = time.time()
        inputs = inputs.to(DEVICE)
        targets = targets.to(DEVICE)
        # target_lens = torch.LongTensor([len(seq) for seq in targets])
       
        optimizer.zero_grad()
        out, out_lens = model(inputs, input_lens)
        loss = criterion(out.transpose(0,1), targets, out_lens, target_lens)
        total_loss = total_loss+loss.item()

        loss.backward()
        optimizer.step()

        del inputs, input_lens, targets, target_lens, out, out_lens, loss
        gc.collect()
        torch.cuda.empty_cache()

        # if batch_idx == 0:
        #     print("Time elapsed", time.time() - first_time)
            
        # if batch_idx % 100 == 0 and batch_idx != 0:
        #     after = time.time()
        #     print("Time: ", after - before)
        #     print("Loss per line: ", loss.item() / batch_idx)
        #     # print("Perplexity: ", np.exp(loss.item() / batch_idx))
        #     after = before
        

    torch.save(model.state_dict(), PATH)
    after = time.time()
    print("loss: {}".format(total_loss))
    print("time elapsed: {}".format(after-before))
    
    with torch.no_grad():    
        model.eval()
        val_loss = 0
        val_loss = val(model, val_loader)
        print("\nValidation loss:",val_loss)
        
    scheduler.step(val_loss)
    print("="*30)

train_epoch(ctcmodel, train_loader, val_loader, 30)

# PATH = "CTCNetwork.pt"
# ctcmodel = Model()
# ctcmodel.load_state_dict(torch.load(PATH))
# ctcmodel.to(DEVICE)

# out_str = []
# with torch.no_grad():
#   ctcmodel.eval()
#   for batch_idx, (inputs, targets, input_lens) in enumerate(test_loader):
#     inputs = inputs.to(DEVICE)
#     targets = targets.to(DEVICE)
#     out, out_lens = ctcmodel(inputs, input_lens)
#     # print(out.shape)
#     # print(out_lens)
#     # break
#     test_Y, _, _, test_Y_lens = ctcdecoder.decode(out, out_lens)   
#     len_inputs = len(inputs) 
#     del inputs, input_lens, targets, out, out_lens
#     gc.collect()
#     torch.cuda.empty_cache()
#     print("decoding finished")
#     for i in range(len_inputs):
#         # For the i-th sample in the batch, get the best output
#         best_seq = test_Y[i, 0, :test_Y_lens[i, 0]]
#         best_pron = ''.join(PHONEME_MAP[i] for i in best_seq)
#         # print(test[i], '->', best_pron)
#         out_str.append(best_pron)
#         del best_seq, best_pron

# ctcdecoder = CTCBeamDecoder(['$'] * (len(PHONEME_MAP)), beam_width=10, log_probs_input=True)
# # 
# test_Y, _, _, test_Y_lens = ctcdecoder.decode(out, out_lens)

